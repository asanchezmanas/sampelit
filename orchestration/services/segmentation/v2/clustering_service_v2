"""
Clustering Service V2
Intelligent clustering with auto-tuning, validation, and retraining.

Copyright (c) 2024 Samplit Technologies
"""

import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from uuid import UUID
import logging
from datetime import datetime, timedelta
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import json

from .cluster_validation import ClusterValidator, ClusteringMetrics

logger = logging.getLogger(__name__)


class ClusteringServiceV2:
    """
    Professional clustering service with auto-tuning
    
    Features:
    - Auto-tuning of n_clusters (Elbow + Silhouette)
    - Quality validation before deployment
    - Model versioning
    - Drift detection
    - Incremental learning
    
    Example:
        >>> service = ClusteringServiceV2(db_pool)
        >>> 
        >>> # Auto-train with optimal n_clusters
        >>> result = await service.auto_train(
        ...     experiment_id,
        ...     feature_vectors
        ... )
        >>> 
        >>> if result['success']:
        ...     print(f"Trained with {result['n_clusters']} clusters")
        ...     print(f"Quality: {result['quality_grade']}")
        ... else:
        ...     print(f"Training failed: {result['reason']}")
    """
    
    def __init__(self, db_pool):
        self.db = db_pool
        self.validator = ClusterValidator()
        self.logger = logging.getLogger(f"{__name__}.ClusteringServiceV2")
        
        # Configuration
        self.min_samples_for_training = 100
        self.min_k = 2
        self.max_k = 8
        self.quality_threshold = 0.3  # Minimum silhouette score
    
    # ========================================================================
    # AUTO-TRAINING
    # ========================================================================
    
    async def auto_train(
        self,
        experiment_id: UUID,
        vectors: List[List[float]],
        force_n_clusters: Optional[int] = None,
        save_model: bool = True
    ) -> Dict[str, Any]:
        """
        Auto-train clustering model with optimal n_clusters
        
        This is the main entry point for training. It automatically:
        1. Validates input data
        2. Finds optimal n_clusters (if not forced)
        3. Trains model
        4. Validates quality
        5. Saves model (if quality acceptable)
        
        Args:
            experiment_id: Experiment UUID
            vectors: List of feature vectors
            force_n_clusters: If set, skip auto-tuning and use this k
            save_model: Whether to save model to database
            
        Returns:
            Dictionary with training results:
            {
                'success': bool,
                'n_clusters': int,
                'quality_metrics': dict,
                'quality_grade': str,
                'model_version': int,
                'reason': str (if failed)
            }
        """
        self.logger.info(
            f"Starting auto-train for experiment {experiment_id} "
            f"with {len(vectors)} samples"
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 1: Validate Input
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        validation = self._validate_input(vectors)
        if not validation['valid']:
            return {
                'success': False,
                'reason': validation['reason']
            }
        
        X = np.array(vectors)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 2: Find Optimal n_clusters (if not forced)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if force_n_clusters:
            optimal_k = force_n_clusters
            self.logger.info(f"Using forced n_clusters = {optimal_k}")
        else:
            self.logger.info("Finding optimal n_clusters...")
            optimal_k = self._find_optimal_k(X)
            self.logger.info(f"âœ… Optimal n_clusters = {optimal_k}")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 3: Train Model with Optimal K
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.logger.info(f"Training K-means with k={optimal_k}...")
        
        kmeans = KMeans(
            n_clusters=optimal_k,
            random_state=42,
            n_init=10,  # Multiple initializations
            max_iter=300
        )
        
        labels = kmeans.fit_predict(X)
        centroids = kmeans.cluster_centers_
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 4: Validate Quality
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.logger.info("Validating clustering quality...")
        
        metrics = self.validator.evaluate(X, labels, centroids)
        
        quality_grade = metrics.get_quality_grade()
        
        self.logger.info(
            f"Quality: {quality_grade} "
            f"(Silhouette: {metrics.silhouette_score:.3f})"
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 5: Check if Quality Acceptable
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not metrics.is_good_quality():
            self.logger.warning(
                f"âš ï¸  Clustering quality poor: {quality_grade}"
            )
            
            # Don't save poor quality models
            return {
                'success': False,
                'reason': 'poor_clustering_quality',
                'n_clusters': optimal_k,
                'quality_metrics': metrics.to_dict(),
                'quality_grade': quality_grade,
                'recommendation': (
                    'Clustering quality is poor. This may indicate:\n'
                    '  - Features are not discriminative enough\n'
                    '  - Data is not naturally clustered\n'
                    '  - Need better feature engineering\n'
                    'Consider using manual segmentation instead.'
                )
            }
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 6: Save Model (if requested and quality OK)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        model_version = None
        
        if save_model:
            self.logger.info("Saving model to database...")
            
            model_version = await self._save_model(
                experiment_id=experiment_id,
                kmeans=kmeans,
                metrics=metrics,
                n_clusters=optimal_k
            )
            
            self.logger.info(f"âœ… Model saved as version {model_version}")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 7: Generate Quality Report
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        report = self.validator.generate_report(metrics)
        self.logger.info(f"\n{report}")
        
        return {
            'success': True,
            'n_clusters': optimal_k,
            'quality_metrics': metrics.to_dict(),
            'quality_grade': quality_grade,
            'model_version': model_version,
            'report': report
        }
    
    # ========================================================================
    # OPTIMAL K SELECTION
    # ========================================================================
    
    def _find_optimal_k(
        self,
        X: np.ndarray,
        method: str = 'silhouette'
    ) -> int:
        """
        Find optimal number of clusters
        
        Uses both Elbow method and Silhouette analysis.
        
        Args:
            X: Feature matrix
            method: 'silhouette' (recommended) or 'elbow'
            
        Returns:
            Optimal k
        """
        n_samples = X.shape[0]
        
        # Adjust k range based on sample size
        max_k = min(self.max_k, n_samples // 20)  # At least 20 samples per cluster
        max_k = max(max_k, self.min_k)
        
        if max_k < self.min_k:
            self.logger.warning(
                f"Not enough samples ({n_samples}) for clustering. "
                f"Minimum {self.min_k * 20} samples recommended."
            )
            return self.min_k
        
        self.logger.info(f"Testing k from {self.min_k} to {max_k}...")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Evaluate each k
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        scores = []
        
        for k in range(self.min_k, max_k + 1):
            try:
                kmeans = KMeans(
                    n_clusters=k,
                    random_state=42,
                    n_init=5,  # Fewer inits for speed
                    max_iter=100
                )
                labels = kmeans.fit_predict(X)
                
                # Calculate metrics
                metrics = self.validator.evaluate(X, labels, kmeans.cluster_centers_)
                
                scores.append({
                    'k': k,
                    'silhouette': metrics.silhouette_score,
                    'davies_bouldin': metrics.davies_bouldin_score,
                    'calinski_harabasz': metrics.calinski_harabasz_score,
                    'inertia': metrics.inertia
                })
                
                self.logger.debug(
                    f"  k={k}: Silhouette={metrics.silhouette_score:.3f}, "
                    f"DB={metrics.davies_bouldin_score:.3f}"
                )
                
            except Exception as e:
                self.logger.warning(f"Failed to evaluate k={k}: {e}")
                continue
        
        if not scores:
            self.logger.error("No valid k values found")
            return self.min_k
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Select optimal k
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if method == 'silhouette':
            # Best silhouette score
            best = max(scores, key=lambda x: x['silhouette'])
            optimal_k = best['k']
            
            self.logger.info(
                f"Silhouette method selected k={optimal_k} "
                f"(score: {best['silhouette']:.3f})"
            )
        
        elif method == 'elbow':
            # Elbow method (look for "elbow" in inertia curve)
            optimal_k = self._find_elbow_point(scores)
            
            self.logger.info(
                f"Elbow method selected k={optimal_k}"
            )
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        return optimal_k
    
    def _find_elbow_point(self, scores: List[Dict]) -> int:
        """
        Find elbow point in inertia curve
        
        Uses the "knee" detection algorithm.
        """
        if len(scores) < 3:
            return scores[0]['k']
        
        # Extract k and inertia
        k_values = [s['k'] for s in scores]
        inertias = [s['inertia'] for s in scores]
        
        # Normalize to [0, 1]
        k_norm = np.array(k_values) - k_values[0]
        k_norm = k_norm / k_norm[-1] if k_norm[-1] > 0 else k_norm
        
        inertia_norm = np.array(inertias) - inertias[-1]
        inertia_norm = inertia_norm / inertia_norm[0] if inertia_norm[0] > 0 else inertia_norm
        
        # Find point with max distance from line connecting first and last
        distances = []
        for i in range(len(k_values)):
            # Distance from point to line
            point = np.array([k_norm[i], inertia_norm[i]])
            line_start = np.array([0, inertia_norm[0]])
            line_end = np.array([1, 0])
            
            dist = np.abs(np.cross(line_end - line_start, point - line_start)) / \
                   np.linalg.norm(line_end - line_start)
            
            distances.append(dist)
        
        # Elbow is the point with maximum distance
        elbow_idx = np.argmax(distances)
        return k_values[elbow_idx]
    
    # ========================================================================
    # PREDICTION
    # ========================================================================
    
    async def predict_cluster(
        self,
        experiment_id: UUID,
        vector: List[float]
    ) -> Optional[int]:
        """
        Predict cluster for a new sample
        
        Args:
            experiment_id: Experiment UUID
            vector: Feature vector
            
        Returns:
            Cluster ID (0-based) or None if no model exists
        """
        # Load model from database
        model_data = await self._load_model(experiment_id)
        
        if not model_data:
            self.logger.warning(
                f"No model found for experiment {experiment_id}"
            )
            return None
        
        # Reconstruct K-means from centroids
        centroids = np.array(model_data['centroids'])
        
        # Predict: assign to nearest centroid
        X = np.array([vector])
        distances = np.linalg.norm(X - centroids, axis=1)
        cluster_id = int(np.argmin(distances))
        
        return cluster_id
    
    # ========================================================================
    # MODEL PERSISTENCE
    # ========================================================================
    
    async def _save_model(
        self,
        experiment_id: UUID,
        kmeans: KMeans,
        metrics: ClusteringMetrics,
        n_clusters: int
    ) -> int:
        """
        Save model to database
        
        Uses JSON (not pickle) for security and portability.
        
        Returns:
            Model version number
        """
        # Serialize model as JSON
        model_json = {
            'centroids': kmeans.cluster_centers_.tolist(),
            'n_clusters': n_clusters,
            'inertia': float(kmeans.inertia_),
            'n_iter': int(kmeans.n_iter_),
            'algorithm': 'kmeans',
            'random_state': 42
        }
        
        # Save to database
        async with self.db.acquire() as conn:
            version = await conn.fetchval(
                """
                INSERT INTO clustering_models (
                    experiment_id,
                    model_data,
                    n_clusters,
                    quality_metrics,
                    created_at
                )
                VALUES ($1, $2, $3, $4, NOW())
                RETURNING version
                """,
                experiment_id,
                json.dumps(model_json),
                n_clusters,
                json.dumps(metrics.to_dict())
            )
        
        return version
    
    async def _load_model(
        self,
        experiment_id: UUID,
        version: Optional[int] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Load model from database
        
        Args:
            experiment_id: Experiment UUID
            version: Specific version to load (None = latest)
            
        Returns:
            Model data dict or None if not found
        """
        async with self.db.acquire() as conn:
            if version is not None:
                row = await conn.fetchrow(
                    """
                    SELECT model_data, quality_metrics, created_at
                    FROM clustering_models
                    WHERE experiment_id = $1 AND version = $2
                    """,
                    experiment_id,
                    version
                )
            else:
                # Get latest version
                row = await conn.fetchrow(
                    """
                    SELECT model_data, quality_metrics, created_at
                    FROM clustering_models
                    WHERE experiment_id = $1
                    ORDER BY version DESC
                    LIMIT 1
                    """,
                    experiment_id
                )
        
        if not row:
            return None
        
        return {
            'model_data': json.loads(row['model_data']),
            'centroids': json.loads(row['model_data'])['centroids'],
            'quality_metrics': json.loads(row['quality_metrics']),
            'created_at': row['created_at']
        }
    
    # ========================================================================
    # DRIFT DETECTION
    # ========================================================================
    
    async def detect_drift(
        self,
        experiment_id: UUID,
        new_vectors: List[List[float]],
        threshold: float = 0.2
    ) -> Dict[str, Any]:
        """
        Detect if data distribution has drifted
        
        Compares new data distribution to original training data.
        
        Args:
            experiment_id: Experiment UUID
            new_vectors: Recent feature vectors
            threshold: Drift threshold (0-1)
            
        Returns:
            Dictionary with drift detection results:
            {
                'drift_detected': bool,
                'drift_score': float,
                'recommendation': str
            }
        """
        self.logger.info(
            f"Detecting drift for experiment {experiment_id} "
            f"with {len(new_vectors)} samples"
        )
        
        # Load current model
        model_data = await self._load_model(experiment_id)
        
        if not model_data:
            return {
                'drift_detected': False,
                'reason': 'no_model_found'
            }
        
        # Predict clusters for new data
        X_new = np.array(new_vectors)
        centroids = np.array(model_data['centroids'])
        
        # Assign to nearest centroid
        distances = np.linalg.norm(
            X_new[:, np.newaxis] - centroids[np.newaxis, :],
            axis=2
        )
        new_labels = np.argmin(distances, axis=1)
        
        # Calculate average distance to assigned centroids
        avg_distance_new = np.mean([
            distances[i, new_labels[i]]
            for i in range(len(new_labels))
        ])
        
        # Compare to original model's inertia per sample
        original_inertia_per_sample = (
            model_data['model_data']['inertia'] / 
            len(new_vectors)  # Approximation
        )
        
        # Drift score: how much worse is the fit
        drift_score = (avg_distance_new - original_inertia_per_sample) / \
                      max(original_inertia_per_sample, 0.001)
        
        drift_detected = drift_score > threshold
        
        result = {
            'drift_detected': drift_detected,
            'drift_score': float(drift_score),
            'threshold': threshold,
            'avg_distance_new': float(avg_distance_new),
            'avg_distance_original': float(original_inertia_per_sample)
        }
        
        if drift_detected:
            result['recommendation'] = (
                f"âš ï¸  Data drift detected (score: {drift_score:.2f} > {threshold}). "
                "Consider retraining the model with recent data."
            )
        else:
            result['recommendation'] = (
                f"âœ… No significant drift (score: {drift_score:.2f} <= {threshold}). "
                "Model is still valid."
            )
        
        self.logger.info(result['recommendation'])
        
        return result
    
    # ========================================================================
    # RE-TRAINING
    # ========================================================================
    
    async def retrain_if_needed(
        self,
        experiment_id: UUID,
        recent_vectors: List[List[float]],
        drift_threshold: float = 0.2,
        min_samples: int = 100
    ) -> Dict[str, Any]:
        """
        Check for drift and retrain if necessary
        
        This can be called periodically (e.g., daily) to keep models fresh.
        
        Args:
            experiment_id: Experiment UUID
            recent_vectors: Recent feature vectors (last 7-30 days)
            drift_threshold: Threshold for triggering retrain
            min_samples: Minimum samples needed to retrain
            
        Returns:
            Dictionary with retrain results
        """
        if len(recent_vectors) < min_samples:
            return {
                'retrained': False,
                'reason': f'insufficient_samples ({len(recent_vectors)} < {min_samples})'
            }
        
        # Check drift
        drift_result = await self.detect_drift(
            experiment_id,
            recent_vectors,
            threshold=drift_threshold
        )
        
        if not drift_result['drift_detected']:
            return {
                'retrained': False,
                'reason': 'no_drift_detected',
                'drift_score': drift_result['drift_score']
            }
        
        # Drift detected - retrain
        self.logger.info("ðŸ”„ Drift detected, retraining model...")
        
        train_result = await self.auto_train(
            experiment_id,
            recent_vectors,
            save_model=True
        )
        
        if train_result['success']:
            return {
                'retrained': True,
                'reason': 'drift_detected',
                'drift_score': drift_result['drift_score'],
                'new_model_version': train_result['model_version'],
                'new_quality_grade': train_result['quality_grade']
            }
        else:
            return {
                'retrained': False,
                'reason': train_result['reason'],
                'drift_score': drift_result['drift_score']
            }
    
    # ========================================================================
    # VALIDATION
    # ========================================================================
    
    def _validate_input(
        self,
        vectors: List[List[float]]
    ) -> Dict[str, Any]:
        """
        Validate input data for training
        
        Returns:
            {'valid': bool, 'reason': str}
        """
        if not vectors:
            return {
                'valid': False,
                'reason': 'No vectors provided'
            }
        
        if len(vectors) < self.min_samples_for_training:
            return {
                'valid': False,
                'reason': (
                    f'Insufficient samples: {len(vectors)} < '
                    f'{self.min_samples_for_training} required'
                )
            }
        
        # Check dimensionality
        n_features = len(vectors[0])
        
        if n_features < 2:
            return {
                'valid': False,
                'reason': f'Too few features: {n_features} < 2'
            }
        
        # Check for all same values
        X = np.array(vectors)
        if np.all(X == X[0]):
            return {
                'valid': False,
                'reason': 'All vectors are identical'
            }
        
        return {'valid': True}


# ============================================================================
# BACKGROUND TASKS
# ============================================================================

async def scheduled_retrain_task(db_pool, experiment_id: UUID):
    """
    Scheduled task to check and retrain models
    
    Run this daily or weekly via cron/scheduler.
    """
    service = ClusteringServiceV2(db_pool)
    
    # Get recent vectors (last 30 days)
    async with db_pool.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT context
            FROM assignments
            WHERE experiment_id = $1
              AND assigned_at > NOW() - INTERVAL '30 days'
            """,
            experiment_id
        )
    
    if not rows:
        logger.info(f"No recent data for {experiment_id}, skipping retrain")
        return
    
    # Extract feature vectors (requires FeatureEngineeringService)
    # This is a placeholder - actual implementation would extract features
    vectors = []  # Extract from contexts
    
    # Retrain if needed
    result = await service.retrain_if_needed(
        experiment_id,
        vectors,
        drift_threshold=0.2,
        min_samples=100
    )
    
    logger.info(f"Retrain task complete for {experiment_id}: {result}")
