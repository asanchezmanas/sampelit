# ğŸ§  Motor de OptimizaciÃ³n (Engine)

**VersiÃ³n**: 1.0  
**Ãšltima actualizaciÃ³n**: Diciembre 2024  
**Nivel**: Intermediate ğŸŸ¡

---

## ğŸ¯ Â¿QuÃ© es el Motor de OptimizaciÃ³n?

El **Engine** es el cerebro algorÃ­tmico de Samplit. Implementa algoritmos de **Multi-Armed Bandit** que:

1. Deciden quÃ© variante mostrar a cada visitante
2. Aprenden de los resultados en tiempo real
3. Optimizan automÃ¡ticamente hacia la variante ganadora

```
Visitante llega
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ENGINE      â”‚ â† Thompson Sampling
â”‚   (Allocator)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Variante  â”‚ â† DecisiÃ³n inteligente
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estructura de Archivos

```
engine/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py           # Exports pÃºblicos
â”‚   â”œâ”€â”€ _base.py              # Clase base abstracta
â”‚   â”œâ”€â”€ allocators/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bayesian.py       # Thompson Sampling (principal)
â”‚   â”‚   â”œâ”€â”€ _bayesian.py      # LÃ³gica matemÃ¡tica
â”‚   â”‚   â”œâ”€â”€ sequential.py     # A/B clÃ¡sico (round-robin)
â”‚   â”‚   â”œâ”€â”€ _explore.py       # Estrategias de exploraciÃ³n
â”‚   â”‚   â””â”€â”€ _registry.py      # Registro de allocators
â”‚   â””â”€â”€ math/
â”‚       â””â”€â”€ statistics.py     # Funciones estadÃ­sticas
â””â”€â”€ state/
    â””â”€â”€ state_manager.py      # GestiÃ³n de estado encriptado
```

---

## ğŸ“„ Archivos Clave Explicados

---

### 1ï¸âƒ£ `_base.py` - Clase Base Abstracta

**PropÃ³sito**: Define la interfaz que todos los allocators deben implementar.

```python
# engine/core/_base.py

"""
Base Allocator - Contrato para todos los algoritmos de asignaciÃ³n.

ğŸ“ PATRÃ“N: Strategy Pattern
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Permite cambiar el algoritmo de asignaciÃ³n sin modificar el resto del cÃ³digo.

ExperimentService no sabe quÃ© allocator usa, solo llama:
    allocator.select(variants)

Esto permite:
- AÃ±adir nuevos algoritmos fÃ¡cilmente
- Testear diferentes estrategias
- Configurar por experimento
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any


class BaseAllocator(ABC):
    """
    Interfaz abstracta para algoritmos de asignaciÃ³n.
    
    Todos los allocators DEBEN implementar:
    - select(): Elegir una variante de la lista
    - update(): Actualizar estado tras una conversiÃ³n
    """
    
    @abstractmethod
    def select(
        self,
        variants: List[Dict[str, Any]],
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Selecciona la mejor variante para mostrar.
        
        Args:
            variants: Lista de variantes disponibles
                [
                    {
                        "id": "var-1",
                        "name": "Control",
                        "total_allocations": 1000,
                        "total_conversions": 80
                    },
                    ...
                ]
            context: Contexto opcional (device, hora, etc.)
        
        Returns:
            La variante seleccionada
        
        ğŸ“ NOTA IMPORTANTE:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Esta funciÃ³n debe ser DETERMINÃSTICA para el mismo estado.
        La aleatoriedad viene de muestrear distribuciones,
        no de random() sin sentido.
        """
        pass
    
    @abstractmethod
    def update(
        self,
        variant_id: str,
        reward: float,
        context: Dict[str, Any] = None
    ) -> None:
        """
        Actualiza el estado del allocator tras observar un resultado.
        
        Args:
            variant_id: ID de la variante que recibiÃ³ el resultado
            reward: Recompensa observada (1.0 = conversiÃ³n, 0.0 = no conversiÃ³n)
            context: Contexto opcional
        
        ğŸ“ APRENDIZAJE EN LÃNEA:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Esta funciÃ³n permite que el allocator "aprenda" de cada interacciÃ³n.
        Con cada conversiÃ³n, actualiza sus "creencias" sobre quÃ© variante es mejor.
        """
        pass
    
    def get_state(self) -> Dict[str, Any]:
        """
        Obtiene el estado interno del allocator.
        
        Ãštil para:
        - Persistencia (guardar en DB)
        - Debugging
        - AuditorÃ­a
        """
        return {}
    
    def set_state(self, state: Dict[str, Any]) -> None:
        """
        Restaura el estado interno del allocator.
        
        Usado al reiniciar el servidor para continuar desde donde estaba.
        """
        pass
```

---

### 2ï¸âƒ£ `bayesian.py` - Thompson Sampling

**PropÃ³sito**: ImplementaciÃ³n principal del algoritmo Thompson Sampling.

```python
# engine/core/allocators/bayesian.py

"""
Thompson Sampling - El algoritmo estrella de Samplit.

ğŸ“ Â¿QUÃ‰ ES THOMPSON SAMPLING?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Un algoritmo de Multi-Armed Bandit que balancea EXPLORACIÃ“N vs EXPLOTACIÃ“N.

Imagina un casino con 3 mÃ¡quinas tragaperras:
- No sabes cuÃ¡l paga mejor
- Cada jugada cuesta dinero
- Â¿CÃ³mo maximizas ganancias?

Estrategias:
1. EXPLORAR: Probar todas por igual para aprender â†’ Lento
2. EXPLOTAR: Solo jugar la que parece mejor â†’ Arriesgado (quizÃ¡s no es la mejor)
3. THOMPSON SAMPLING: Balance inteligente â†’ Ã“ptimo

CÃ³mo funciona:
1. Cada variante tiene una "creencia" modelada como distribuciÃ³n Beta
2. Muestreamos un valor de cada distribuciÃ³n
3. Elegimos la variante con el mayor valor muestreado
4. Observamos resultado y actualizamos creencias

La magia: Variantes con mÃ¡s incertidumbre tienen mÃ¡s probabilidad
de ser elegidas (exploraciÃ³n), pero variantes que funcionan bien
tambiÃ©n tienen alta probabilidad (explotaciÃ³n).
"""

import numpy as np
from typing import List, Dict, Any, Optional
import logging
from ._base import BaseAllocator

logger = logging.getLogger(__name__)


class BayesianAllocator(BaseAllocator):
    """
    Thompson Sampling con distribuciÃ³n Beta.
    
    ğŸ“ DISTRIBUCIÃ“N BETA:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - Describe la probabilidad de un evento binario (Ã©xito/fracaso)
    - ParÃ¡metros: Î± (alpha) y Î² (beta)
    - Î± = nÃºmero de Ã©xitos + 1
    - Î² = nÃºmero de fracasos + 1
    
    Propiedades:
    - Media = Î± / (Î± + Î²)
    - A mÃ¡s datos â†’ DistribuciÃ³n mÃ¡s "apretada" (menos incertidumbre)
    - Al inicio (Î±=1, Î²=1) â†’ DistribuciÃ³n uniforme (mÃ¡xima incertidumbre)
    
    Ejemplo visual:
    
    Pocos datos (Î±=3, Î²=7):
            â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‡â–†â–…â–„â–ƒâ–‚
        0%  10% 20% 30% 40%   â† Amplio rango de posibilidades
    
    Muchos datos (Î±=30, Î²=70):
              â–‚â–„â–ˆâ–„â–‚
        0%  10% 20% 30% 40%   â† MÃ¡s certeza sobre el valor real
    """
    
    def __init__(
        self,
        prior_alpha: float = 1.0,
        prior_beta: float = 1.0,
        min_samples: int = 0
    ):
        """
        Inicializa el allocator.
        
        Args:
            prior_alpha: Prior Î± (default 1.0 = uniforme)
            prior_beta: Prior Î² (default 1.0 = uniforme)
            min_samples: MÃ­nimo de muestras antes de optimizar
                         (Ãºtil si quieres fase inicial 50/50)
        
        ğŸ“ Â¿QUÃ‰ SON LOS PRIORS?
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Los priors representan tu "creencia inicial" antes de ver datos.
        
        prior_alpha=1, prior_beta=1 â†’ "No sÃ© nada" (uniforme)
        prior_alpha=10, prior_beta=10 â†’ "Creo que CR ~50%, pero no estoy seguro"
        prior_alpha=1, prior_beta=9 â†’ "Creo que CR ~10%"
        
        En general, priors uniformes (1,1) son la opciÃ³n mÃ¡s neutral.
        """
        self.prior_alpha = prior_alpha
        self.prior_beta = prior_beta
        self.min_samples = min_samples
        
        # Estado interno: estadÃ­sticas por variante
        self._variant_stats: Dict[str, Dict[str, float]] = {}
    
    def select(
        self,
        variants: List[Dict[str, Any]],
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Selecciona una variante usando Thompson Sampling.
        
        ğŸ“ ALGORITMO PASO A PASO:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        1. Para cada variante, calculamos Î± y Î²:
           Î± = conversiones + prior_alpha
           Î² = (visitas - conversiones) + prior_beta
        
        2. Muestreamos un valor de Beta(Î±, Î²):
           sample ~ Beta(Î±, Î²)
        
        3. Elegimos la variante con el sample mÃ¡s alto
        
        Ejemplo con 3 variantes:
        
        Variante A: 100 visitas, 8 conversiones
          Î± = 8 + 1 = 9
          Î² = 92 + 1 = 93
          Sample: 0.07 (sacado de Beta(9, 93))
        
        Variante B: 100 visitas, 12 conversiones
          Î± = 12 + 1 = 13
          Î² = 88 + 1 = 89
          Sample: 0.15 (sacado de Beta(13, 89))
        
        Variante C: 50 visitas, 5 conversiones (menos datos)
          Î± = 5 + 1 = 6
          Î² = 45 + 1 = 46
          Sample: 0.18 (mayor incertidumbre â†’ mÃ¡s variabilidad)
        
        â†’ Elegimos C (0.18 > 0.15 > 0.07)
        
        Nota: C tiene menos datos, asÃ­ que su sample puede ser muy alto
        o muy bajo. Esto es la EXPLORACIÃ“N automÃ¡tica.
        """
        
        if not variants:
            raise ValueError("No variants provided")
        
        # Caso especial: pocas muestras â†’ distribuciÃ³n uniforme
        total_samples = sum(v.get('total_allocations', 0) for v in variants)
        if total_samples < self.min_samples:
            # SelecciÃ³n aleatoria uniforme (exploraciÃ³n pura)
            return np.random.choice(variants)
        
        # Thompson Sampling
        samples = []
        
        for variant in variants:
            variant_id = variant.get('id')
            allocations = variant.get('total_allocations', 0)
            conversions = variant.get('total_conversions', 0)
            
            # Calcular parÃ¡metros Beta
            alpha = conversions + self.prior_alpha
            beta = (allocations - conversions) + self.prior_beta
            
            # Validar parÃ¡metros (deben ser > 0)
            alpha = max(alpha, 0.01)
            beta = max(beta, 0.01)
            
            # Muestrear de la distribuciÃ³n Beta
            sample = np.random.beta(alpha, beta)
            
            samples.append({
                'variant': variant,
                'sample': sample,
                'alpha': alpha,
                'beta': beta,
                'expected_cr': alpha / (alpha + beta)  # Media de la distribuciÃ³n
            })
            
            logger.debug(
                f"Variante {variant.get('name')}: "
                f"Î±={alpha:.1f}, Î²={beta:.1f}, "
                f"E[CR]={alpha/(alpha+beta):.3f}, "
                f"sample={sample:.4f}"
            )
        
        # Seleccionar variante con mayor sample
        winner = max(samples, key=lambda x: x['sample'])
        
        logger.debug(f"Variante seleccionada: {winner['variant'].get('name')}")
        
        return winner['variant']
    
    def update(
        self,
        variant_id: str,
        reward: float,
        context: Dict[str, Any] = None
    ) -> None:
        """
        Actualiza estadÃ­sticas tras observar un resultado.
        
        ğŸ“ APRENDIZAJE BAYESIANO:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Cada observaciÃ³n actualiza nuestra "creencia" sobre la variante.
        
        Si reward=1 (conversiÃ³n): Î± aumenta â†’ Mayor probabilidad estimada
        Si reward=0 (no conversiÃ³n): Î² aumenta â†’ Menor probabilidad estimada
        
        Esto es el "Bayesian posterior update":
        Prior: Beta(Î±, Î²)
        + ObservaciÃ³n
        = Posterior: Beta(Î± + reward, Î² + (1 - reward))
        """
        
        if variant_id not in self._variant_stats:
            self._variant_stats[variant_id] = {
                'alpha': self.prior_alpha,
                'beta': self.prior_beta,
                'total_samples': 0,
                'total_rewards': 0
            }
        
        stats = self._variant_stats[variant_id]
        stats['total_samples'] += 1
        stats['total_rewards'] += reward
        
        # Actualizar parÃ¡metros Beta
        if reward > 0:
            stats['alpha'] += reward
        else:
            stats['beta'] += (1 - reward)
        
        logger.debug(
            f"Updated variant {variant_id}: "
            f"Î±={stats['alpha']:.1f}, Î²={stats['beta']:.1f}"
        )
    
    def get_variant_probability(
        self,
        variant: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        Calcula estadÃ­sticas de probabilidad para una variante.
        
        Returns:
            {
                "expected_cr": 0.085,      # Tasa de conversiÃ³n esperada
                "ci_lower": 0.065,         # Intervalo de confianza inferior
                "ci_upper": 0.110,         # Intervalo de confianza superior
                "uncertainty": 0.045       # Ancho del intervalo (incertidumbre)
            }
        """
        from scipy import stats as scipy_stats
        
        allocations = variant.get('total_allocations', 0)
        conversions = variant.get('total_conversions', 0)
        
        alpha = conversions + self.prior_alpha
        beta = (allocations - conversions) + self.prior_beta
        
        # Media de la distribuciÃ³n Beta
        expected_cr = alpha / (alpha + beta)
        
        # Intervalo de credibilidad del 95% (equivalente Bayesiano al CI)
        ci_lower = scipy_stats.beta.ppf(0.025, alpha, beta)
        ci_upper = scipy_stats.beta.ppf(0.975, alpha, beta)
        
        return {
            "expected_cr": round(expected_cr, 4),
            "ci_lower": round(ci_lower, 4),
            "ci_upper": round(ci_upper, 4),
            "uncertainty": round(ci_upper - ci_lower, 4),
            "alpha": alpha,
            "beta": beta
        }
    
    def calculate_win_probability(
        self,
        variants: List[Dict[str, Any]],
        n_simulations: int = 10000
    ) -> Dict[str, float]:
        """
        Calcula la probabilidad de que cada variante sea la mejor.
        
        ğŸ“ SIMULACIÃ“N MONTE CARLO:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. Simulamos N "mundos posibles"
        2. En cada mundo, muestreamos de cada distribuciÃ³n Beta
        3. Contamos en cuÃ¡ntos mundos gana cada variante
        4. win_probability = victorias / N
        
        Ejemplo con N=10,000:
        - Variante A gana en 3,200 mundos â†’ 32% prob de ser mejor
        - Variante B gana en 6,800 mundos â†’ 68% prob de ser mejor
        
        Args:
            variants: Lista de variantes con estadÃ­sticas
            n_simulations: NÃºmero de simulaciones (mÃ¡s = mÃ¡s preciso)
        
        Returns:
            {"var-1": 0.32, "var-2": 0.68}
        """
        
        n_variants = len(variants)
        if n_variants == 0:
            return {}
        
        # Preparar parÃ¡metros
        alphas = []
        betas = []
        variant_ids = []
        
        for v in variants:
            allocations = v.get('total_allocations', 0)
            conversions = v.get('total_conversions', 0)
            
            alphas.append(conversions + self.prior_alpha)
            betas.append((allocations - conversions) + self.prior_beta)
            variant_ids.append(v.get('id'))
        
        # Generar muestras (matriz n_simulations x n_variants)
        samples = np.zeros((n_simulations, n_variants))
        for i in range(n_variants):
            samples[:, i] = np.random.beta(alphas[i], betas[i], n_simulations)
        
        # Encontrar ganador en cada simulaciÃ³n
        winners = np.argmax(samples, axis=1)
        
        # Contar victorias
        win_counts = np.bincount(winners, minlength=n_variants)
        win_probs = win_counts / n_simulations
        
        return {
            variant_ids[i]: round(float(win_probs[i]), 4)
            for i in range(n_variants)
        }
    
    def get_state(self) -> Dict[str, Any]:
        """Serializa el estado para persistencia."""
        return {
            'prior_alpha': self.prior_alpha,
            'prior_beta': self.prior_beta,
            'min_samples': self.min_samples,
            'variant_stats': self._variant_stats
        }
    
    def set_state(self, state: Dict[str, Any]) -> None:
        """Restaura el estado desde persistencia."""
        self.prior_alpha = state.get('prior_alpha', 1.0)
        self.prior_beta = state.get('prior_beta', 1.0)
        self.min_samples = state.get('min_samples', 0)
        self._variant_stats = state.get('variant_stats', {})
```

---

### 3ï¸âƒ£ `sequential.py` - A/B Testing ClÃ¡sico

**PropÃ³sito**: Allocator simple para A/B testing tradicional (sin optimizaciÃ³n).

```python
# engine/core/allocators/sequential.py

"""
Sequential Allocator - A/B Testing clÃ¡sico.

ğŸ“ Â¿CUÃNDO USAR ESTO EN VEZ DE THOMPSON SAMPLING?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Thompson Sampling es casi siempre mejor, PERO:

1. Tests cortos (< 1 semana): No hay tiempo para que TS aprenda
2. Requisitos de muestra iguales: Si necesitas exactamente 50/50
3. ComparaciÃ³n con histÃ³ricos: Si quieres comparar con datos anteriores
4. Regulaciones: Algunos sectores requieren distribuciÃ³n uniforme

En la prÃ¡ctica, ~95% de los experimentos deberÃ­an usar Thompson Sampling.
"""

import random
from typing import List, Dict, Any
from ._base import BaseAllocator


class SequentialAllocator(BaseAllocator):
    """
    DistribuciÃ³n uniforme (round-robin ponderado).
    
    Cada variante tiene un "peso" y la distribuciÃ³n es proporcional.
    Peso 1:1 â†’ 50/50
    Peso 2:1 â†’ 66/33
    """
    
    def __init__(self, weights: Dict[str, float] = None):
        """
        Args:
            weights: Pesos por variante {"var-1": 1.0, "var-2": 1.0}
                     Si None, distribuciÃ³n uniforme
        """
        self.weights = weights or {}
    
    def select(
        self,
        variants: List[Dict[str, Any]],
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Selecciona variante con distribuciÃ³n ponderada.
        
        ğŸ“ ALGORITMO:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. Asignar pesos (default = 1.0 para todos)
        2. Normalizar pesos a probabilidades
        3. SelecciÃ³n aleatoria ponderada
        """
        
        if not variants:
            raise ValueError("No variants provided")
        
        # Obtener pesos
        weights = []
        for v in variants:
            variant_id = v.get('id')
            weight = self.weights.get(variant_id, 1.0)
            weights.append(weight)
        
        # Normalizar a probabilidades
        total_weight = sum(weights)
        if total_weight == 0:
            probabilities = [1/len(variants)] * len(variants)
        else:
            probabilities = [w / total_weight for w in weights]
        
        # SelecciÃ³n aleatoria ponderada
        cumulative = 0
        r = random.random()
        
        for i, prob in enumerate(probabilities):
            cumulative += prob
            if r <= cumulative:
                return variants[i]
        
        # Fallback (no deberÃ­a llegar aquÃ­)
        return variants[-1]
    
    def update(
        self,
        variant_id: str,
        reward: float,
        context: Dict[str, Any] = None
    ) -> None:
        """
        No hace nada - este allocator no aprende.
        
        ğŸ“ DIFERENCIA CLAVE CON THOMPSON SAMPLING:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        SequentialAllocator: DistribuciÃ³n fija, no cambia
        BayesianAllocator: Aprende y adapta la distribuciÃ³n
        """
        pass  # Este allocator no aprende
```

---

## ğŸ”¢ ComparaciÃ³n de Algoritmos

| Aspecto | Sequential (A/B clÃ¡sico) | Thompson Sampling |
|---------|-------------------------|-------------------|
| **DistribuciÃ³n** | Fija (ej: 50/50) | DinÃ¡mica (aprende) |
| **ExploraciÃ³n** | Ninguna | AutomÃ¡tica |
| **ExplotaciÃ³n** | Ninguna | AutomÃ¡tica |
| **Velocidad de aprendizaje** | N/A | RÃ¡pida |
| **Regret** | Alto | Bajo |
| **CuÃ¡ndo usar** | Tests cortos, regulaciÃ³n | Siempre que sea posible |

ğŸ“ **"Regret"**: PÃ©rdida acumulada por no mostrar siempre la mejor variante.
Thompson Sampling minimiza el regret al balancear exploraciÃ³n y explotaciÃ³n.

---

## ğŸ“Š Ejemplo PrÃ¡ctico

```python
# Ejemplo de uso del motor

from engine.core.allocators.bayesian import BayesianAllocator

# Crear allocator
allocator = BayesianAllocator()

# Datos de variantes
variants = [
    {"id": "var-1", "name": "Control", "total_allocations": 1000, "total_conversions": 80},
    {"id": "var-2", "name": "Variante B", "total_allocations": 1000, "total_conversions": 120},
]

# Seleccionar variante para un visitante
selected = allocator.select(variants)
print(f"Mostrar: {selected['name']}")

# Calcular probabilidades de ganar
win_probs = allocator.calculate_win_probability(variants)
print(f"Prob de ganar: {win_probs}")
# Output: {"var-1": 0.02, "var-2": 0.98}
```

---

## ğŸ“š Recursos Adicionales

Para profundizar en la teorÃ­a:

1. **Thompson Sampling Tutorial**: [Google Research Paper](https://arxiv.org/abs/1707.02038)
2. **Multi-Armed Bandits Book**: "Bandit Algorithms" by Lattimore & SzepesvÃ¡ri
3. **Beta Distribution**: [Interactive Visualization](https://seeing-theory.brown.edu/)

**PrÃ³ximo paso**: [Ver Scripts de Mantenimiento](./scripts.md)

